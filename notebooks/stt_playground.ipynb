{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from collections import Counter\n",
    "from datasets import load_dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Preprocessing\n",
    "\n",
    "class TextProcessor:\n",
    "    \"\"\"\n",
    "    Handles conversion between text and numerical sequences for training.\n",
    "    This is crucial because neural networks work with numbers, not text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size_limit=None):\n",
    "        # Special tokens that help the model understand structure\n",
    "        self.pad_token = '<PAD>'    # Padding for variable-length sequences\n",
    "        self.unk_token = '<UNK>'    # Unknown/out-of-vocabulary words\n",
    "        self.sos_token = '<SOS>'    # Start of sequence\n",
    "        self.eos_token = '<EOS>'    # End of sequence\n",
    "\n",
    "        # Will store character-to-index and index-to-character mappings\n",
    "        self.char_to_idx = {}\n",
    "        self.idx_to_char = {}\n",
    "        self.vocab_size = 0\n",
    "        self.vocab_size_limit = vocab_size_limit\n",
    "\n",
    "    def build_vocabulary(self, texts):\n",
    "        \"\"\"\n",
    "        Creates a mapping between characters and numbers.\n",
    "        The model needs to convert text to numbers for processing.\n",
    "        \"\"\"\n",
    "        # Count frequency of each character across all texts\n",
    "        char_counter = Counter()\n",
    "        for text in texts:\n",
    "            # Convert to lowercase and count each character\n",
    "            char_counter.update(text.lower())\n",
    "\n",
    "        # Start with special tokens\n",
    "        vocab = [self.pad_token, self.unk_token, self.sos_token, self.eos_token]\n",
    "\n",
    "        # Add most common characters (if limit specified)\n",
    "        if self.vocab_size_limit:\n",
    "            most_common_chars = char_counter.most_common(self.vocab_size_limit - len(vocab))\n",
    "            vocab.extend([char for char, _ in most_common_chars])\n",
    "        else:\n",
    "            vocab.extend(list(char_counter.keys()))\n",
    "\n",
    "        # Create bidirectional mapping\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
    "        self.vocab_size = len(vocab)\n",
    "\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Sample characters: {list(self.char_to_idx.keys())[:20]}\")\n",
    "\n",
    "    def text_to_sequence(self, text):\n",
    "        \"\"\"Convert text string to sequence of integers\"\"\"\n",
    "        sequence = [self.char_to_idx[self.sos_token]]  # Start token\n",
    "\n",
    "        for char in text.lower():\n",
    "            # Use unknown token if character not in vocabulary\n",
    "            idx = self.char_to_idx.get(char, self.char_to_idx[self.unk_token])\n",
    "            sequence.append(idx)\n",
    "\n",
    "        sequence.append(self.char_to_idx[self.eos_token])  # End token\n",
    "        return sequence\n",
    "\n",
    "    def sequence_to_text(self, sequence):\n",
    "        \"\"\"Convert sequence of integers back to text\"\"\"\n",
    "        chars = []\n",
    "        for idx in sequence:\n",
    "            if idx in self.idx_to_char:\n",
    "                char = self.idx_to_char[idx]\n",
    "                # Skip special tokens in output\n",
    "                if char not in [self.pad_token, self.sos_token, self.eos_token]:\n",
    "                    chars.append(char)\n",
    "        return ''.join(chars)"
   ],
   "id": "1ce91f3ddbbd6e66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T11:16:44.450977Z",
     "start_time": "2025-08-22T11:16:44.442988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Audio Processing\n",
    "\n",
    "class AudioProcessor:\n",
    "    \"\"\"\n",
    "    Handles audio loading and feature extraction.\n",
    "    Converts raw audio waveforms into features the model can understand.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate=16000, n_mels=80, n_fft=512, hop_length=256):\n",
    "        self.sample_rate = sample_rate  # Standard rate for speech recognition\n",
    "        self.n_mels = n_mels           # Number of mel-frequency bands\n",
    "        self.n_fft = n_fft             # FFT window size\n",
    "        self.hop_length = hop_length   # Step size between windows\n",
    "\n",
    "        # Mel-spectrogram transform - converts audio to visual representation\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_mels=n_mels,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length\n",
    "        )\n",
    "\n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"\n",
    "        Load audio file and convert to standard format.\n",
    "        Ensures all audio has consistent sample rate and format.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio using torchaudio\n",
    "            waveform, orig_sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "            # Convert to mono if stereo\n",
    "            if waveform.size(0) > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "            # Resample if necessary\n",
    "            if orig_sample_rate != self.sample_rate:\n",
    "                resampler = torchaudio.transforms.Resample(\n",
    "                    orig_freq=orig_sample_rate,\n",
    "                    new_freq=self.sample_rate\n",
    "                )\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            return waveform.squeeze(0)  # Remove channel dimension\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_features(self, waveform):\n",
    "        \"\"\"\n",
    "        Convert audio waveform to mel-spectrogram features.\n",
    "        Mel-spectrograms capture the frequency content over time,\n",
    "        similar to how humans perceive sound.\n",
    "        \"\"\"\n",
    "        # Apply mel-spectrogram transformation\n",
    "        mel_spec = self.mel_transform(waveform.unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "        # Convert to log scale (more natural for human hearing)\n",
    "        mel_spec = torch.log(mel_spec + 1e-8)  # Add small value to avoid log(0)\n",
    "\n",
    "        # Remove batch dimension and transpose for model input\n",
    "        # Shape: (time_steps, n_mels)\n",
    "        return mel_spec.squeeze(0).transpose(0, 1)"
   ],
   "id": "a289d3e9e48d31f8",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T11:16:48.109822Z",
     "start_time": "2025-08-22T11:16:48.095925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset\n",
    "\n",
    "class CommonVoiceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset class for Mozilla Common Voice data.\n",
    "    Handles loading audio files and corresponding transcriptions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, metadata_file, text_processor, audio_processor, max_audio_length=16000*10):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.text_processor = text_processor\n",
    "        self.audio_processor = audio_processor\n",
    "        self.max_audio_length = max_audio_length  # 10 seconds at 16kHz\n",
    "\n",
    "        # Load metadata CSV file\n",
    "        self.metadata = pd.read_csv(metadata_file, sep='\\t')\n",
    "\n",
    "        # Filter out very long audio files (for memory efficiency)\n",
    "        print(f\"Total samples before filtering: {len(self.metadata)}\")\n",
    "        self.metadata = self.metadata[self.metadata['path'].notna()]\n",
    "        print(f\"Total samples after filtering: {len(self.metadata)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a single training example: audio features and text sequence.\n",
    "        This method is called by PyTorch's DataLoader during training.\n",
    "        \"\"\"\n",
    "        row = self.metadata.iloc[idx]\n",
    "\n",
    "        # Get file paths\n",
    "        audio_path = self.data_dir / \"clips\" / row['path']\n",
    "        text = row['sentence']\n",
    "\n",
    "        # Load and process audio\n",
    "        waveform = self.audio_processor.load_audio(audio_path)\n",
    "\n",
    "        if waveform is None:\n",
    "            # Return dummy data if audio loading fails\n",
    "            return torch.zeros(1, self.audio_processor.n_mels), torch.tensor([0])\n",
    "\n",
    "        # Truncate very long audio\n",
    "        if len(waveform) > self.max_audio_length:\n",
    "            waveform = waveform[:self.max_audio_length]\n",
    "\n",
    "        # Extract features\n",
    "        features = self.audio_processor.extract_features(waveform)\n",
    "\n",
    "        # Convert text to sequence\n",
    "        text_sequence = torch.tensor(self.text_processor.text_to_sequence(text))\n",
    "\n",
    "        return features, text_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom function to handle variable-length sequences in batches.\n",
    "    Pads shorter sequences to match the longest in each batch.\n",
    "    \"\"\"\n",
    "    # Separate audio features and text sequences\n",
    "    features, texts = zip(*batch)\n",
    "\n",
    "    # Pad audio features to same length\n",
    "    features_lengths = [f.size(0) for f in features]\n",
    "    max_feature_length = max(features_lengths)\n",
    "\n",
    "    padded_features = []\n",
    "    for f in features:\n",
    "        # Pad with zeros if shorter than max length\n",
    "        if f.size(0) < max_feature_length:\n",
    "            pad_size = max_feature_length - f.size(0)\n",
    "            f = torch.cat([f, torch.zeros(pad_size, f.size(1))], dim=0)\n",
    "        padded_features.append(f)\n",
    "\n",
    "    # Stack into batch tensor\n",
    "    features_batch = torch.stack(padded_features)\n",
    "\n",
    "    # Pad text sequences\n",
    "    text_lengths = [len(t) for t in texts]\n",
    "    max_text_length = max(text_lengths)\n",
    "\n",
    "    padded_texts = []\n",
    "    for t in texts:\n",
    "        if len(t) < max_text_length:\n",
    "            # Pad with pad token index (0)\n",
    "            padding = torch.zeros(max_text_length - len(t), dtype=torch.long)\n",
    "            t = torch.cat([t, padding])\n",
    "        padded_texts.append(t)\n",
    "\n",
    "    texts_batch = torch.stack(padded_texts)\n",
    "\n",
    "    return features_batch, texts_batch, torch.tensor(features_lengths), torch.tensor(text_lengths)"
   ],
   "id": "5d95ffa7a13b1ad0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T11:16:50.297215Z",
     "start_time": "2025-08-22T11:16:50.290831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HuggingFaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset wrapper for Hugging Face CoVoST 2 dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset, text_processor, audio_processor, max_audio_length=16000*10):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.text_processor = text_processor\n",
    "        self.audio_processor = audio_processor\n",
    "        self.max_audio_length = max_audio_length\n",
    "\n",
    "        print(f\"Total samples: {len(self.hf_dataset)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a single training example: audio features and text sequence.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sample = self.hf_dataset[idx]\n",
    "\n",
    "            # Get audio data directly from HF dataset\n",
    "            audio_array = sample['audio']['array']\n",
    "            text = sample['sentence']\n",
    "\n",
    "            # Convert numpy array to torch tensor\n",
    "            waveform = torch.tensor(audio_array, dtype=torch.float32)\n",
    "\n",
    "            # Truncate very long audio\n",
    "            if len(waveform) > self.max_audio_length:\n",
    "                waveform = waveform[:self.max_audio_length]\n",
    "\n",
    "            # Extract features\n",
    "            features = self.audio_processor.extract_features(waveform)\n",
    "\n",
    "            # Convert text to sequence\n",
    "            text_sequence = torch.tensor(self.text_processor.text_to_sequence(text))\n",
    "\n",
    "            return features, text_sequence\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            # Return dummy data if there's an error\n",
    "            return torch.zeros(1, self.audio_processor.n_mels), torch.tensor([0])"
   ],
   "id": "9a40a8ece3c0fc54",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T11:16:52.048550Z",
     "start_time": "2025-08-22T11:16:52.031321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Network\n",
    "\n",
    "class AttentionMechanism(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism allows the model to focus on relevant parts\n",
    "    of the audio when predicting each character.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_hidden_size, decoder_hidden_size):\n",
    "        super().__init__()\n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.decoder_hidden_size = decoder_hidden_size\n",
    "\n",
    "        # Linear layers for computing attention scores\n",
    "        self.attention = nn.Linear(encoder_hidden_size + decoder_hidden_size, decoder_hidden_size)\n",
    "        self.v = nn.Linear(decoder_hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        \"\"\"\n",
    "        encoder_outputs: (batch_size, seq_len, encoder_hidden_size)\n",
    "        decoder_hidden: (batch_size, decoder_hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = encoder_outputs.size()\n",
    "\n",
    "        # Repeat decoder hidden for each encoder output\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        # Concatenate encoder outputs with decoder hidden\n",
    "        combined = torch.cat([encoder_outputs, decoder_hidden], dim=2)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attention_scores = self.v(torch.tanh(self.attention(combined)))\n",
    "        attention_weights = F.softmax(attention_scores.squeeze(2), dim=1)\n",
    "\n",
    "        # Apply attention weights to encoder outputs\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "\n",
    "        return context.squeeze(1), attention_weights\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder processes audio features and creates a representation\n",
    "    that captures the acoustic information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Bidirectional LSTM to capture context from both directions\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Project bidirectional output back to hidden_size\n",
    "        self.projection = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "    def forward(self, features, feature_lengths):\n",
    "        \"\"\"\n",
    "        features: (batch_size, seq_len, input_size)\n",
    "        feature_lengths: actual lengths before padding\n",
    "        \"\"\"\n",
    "        # Pack sequences for efficient processing\n",
    "        packed_features = nn.utils.rnn.pack_padded_sequence(\n",
    "            features, feature_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # Process through LSTM\n",
    "        packed_outputs, (hidden, cell) = self.lstm(packed_features)\n",
    "\n",
    "        # Unpack sequences\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "\n",
    "        # Project to desired hidden size\n",
    "        outputs = self.projection(outputs)\n",
    "\n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder generates text character by character,\n",
    "    using attention to focus on relevant audio parts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size, encoder_hidden_size, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Character embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        # LSTM for sequential processing\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_size + encoder_hidden_size,  # embedding + context\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = AttentionMechanism(encoder_hidden_size, hidden_size)\n",
    "\n",
    "        # Output projection to vocabulary\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, encoder_outputs, target_sequence=None, max_length=100):\n",
    "        \"\"\"\n",
    "        encoder_outputs: (batch_size, seq_len, encoder_hidden_size)\n",
    "        target_sequence: (batch_size, target_len) - for training\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        device = encoder_outputs.device\n",
    "\n",
    "        # Initialize decoder hidden state\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        decoder_state = (hidden, cell)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        if target_sequence is not None:  # Training mode\n",
    "            target_length = target_sequence.size(1)\n",
    "\n",
    "            for t in range(target_length - 1):  # Exclude last token\n",
    "                # Current input character\n",
    "                if t == 0:\n",
    "                    # Start with SOS token\n",
    "                    current_input = torch.full((batch_size,), 2, device=device, dtype=torch.long)  # SOS = 2\n",
    "                else:\n",
    "                    current_input = target_sequence[:, t]\n",
    "\n",
    "                # Get embedding\n",
    "                embedded = self.embedding(current_input)  # (batch_size, hidden_size)\n",
    "\n",
    "                # Calculate attention and context\n",
    "                context, _ = self.attention(encoder_outputs, decoder_state[0][-1])\n",
    "\n",
    "                # Combine embedding and context\n",
    "                lstm_input = torch.cat([embedded, context], dim=1).unsqueeze(1)\n",
    "\n",
    "                # Process through LSTM\n",
    "                lstm_output, decoder_state = self.lstm(lstm_input, decoder_state)\n",
    "                lstm_output = self.dropout(lstm_output.squeeze(1))\n",
    "\n",
    "                # Generate output distribution\n",
    "                output = self.out(lstm_output)\n",
    "                outputs.append(output)\n",
    "\n",
    "            return torch.stack(outputs, dim=1)\n",
    "\n",
    "        else:  # Inference mode\n",
    "            current_input = torch.full((batch_size,), 2, device=device, dtype=torch.long)  # SOS\n",
    "\n",
    "            for t in range(max_length):\n",
    "                embedded = self.embedding(current_input)\n",
    "                context, _ = self.attention(encoder_outputs, decoder_state[0][-1])\n",
    "                lstm_input = torch.cat([embedded, context], dim=1).unsqueeze(1)\n",
    "\n",
    "                lstm_output, decoder_state = self.lstm(lstm_input, decoder_state)\n",
    "                output = self.out(lstm_output.squeeze(1))\n",
    "\n",
    "                outputs.append(output)\n",
    "\n",
    "                # Get next input (greedy decoding)\n",
    "                current_input = output.argmax(dim=1)\n",
    "\n",
    "                # Stop if all sequences generated EOS token\n",
    "                if (current_input == 3).all():  # EOS = 3\n",
    "                    break\n",
    "\n",
    "            return torch.stack(outputs, dim=1)\n",
    "\n",
    "class Speech2TextModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Speech-to-Text model combining encoder and decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, input_size=80, encoder_hidden=256, decoder_hidden=256, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            input_size=input_size,\n",
    "            hidden_size=encoder_hidden,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=decoder_hidden,\n",
    "            encoder_hidden_size=encoder_hidden,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, features, feature_lengths, target_sequence=None):\n",
    "        # Encode audio features\n",
    "        encoder_outputs, _ = self.encoder(features, feature_lengths)\n",
    "\n",
    "        # Decode to text\n",
    "        decoder_outputs = self.decoder(encoder_outputs, target_sequence)\n",
    "\n",
    "        return decoder_outputs"
   ],
   "id": "59c050e2c1db6f9d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T11:16:53.205820Z",
     "start_time": "2025-08-22T11:16:53.194962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training\n",
    "\n",
    "def train_model(model, train_loader, val_loader, text_processor, num_epochs=10, device='cpu'):\n",
    "    \"\"\"\n",
    "    Training loop for the speech-to-text model.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Loss function - CrossEntropyLoss for character prediction\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
    "\n",
    "    # Optimizer - Adam is good for sequence models\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "\n",
    "        for batch_idx, (features, texts, feature_lengths, text_lengths) in enumerate(train_loader):\n",
    "            features = features.to(device)\n",
    "            texts = texts.to(device)\n",
    "            feature_lengths = feature_lengths.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features, feature_lengths, texts)\n",
    "\n",
    "            # Calculate loss\n",
    "            # Reshape for loss calculation\n",
    "            outputs = outputs.reshape(-1, outputs.size(-1))\n",
    "            targets = texts[:, 1:].reshape(-1)  # Exclude SOS token\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for features, texts, feature_lengths, text_lengths in val_loader:\n",
    "                features = features.to(device)\n",
    "                texts = texts.to(device)\n",
    "                feature_lengths = feature_lengths.to(device)\n",
    "\n",
    "                outputs = model(features, feature_lengths, texts)\n",
    "\n",
    "                outputs = outputs.reshape(-1, outputs.size(-1))\n",
    "                targets = texts[:, 1:].reshape(-1)\n",
    "\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "\n",
    "        print(f'Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'New best model saved with validation loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "def predict_text(model, audio_file, text_processor, audio_processor, device='cpu'):\n",
    "    \"\"\"\n",
    "    Predict text from audio file using trained model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Load and process audio\n",
    "    waveform = audio_processor.load_audio(audio_file)\n",
    "    if waveform is None:\n",
    "        return \"Error loading audio\"\n",
    "\n",
    "    features = audio_processor.extract_features(waveform)\n",
    "    features = features.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    feature_lengths = torch.tensor([features.size(1)]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate text\n",
    "        outputs = model(features, feature_lengths)\n",
    "\n",
    "        # Convert to text\n",
    "        predicted_sequence = outputs.argmax(dim=-1).squeeze(0)\n",
    "        predicted_text = text_processor.sequence_to_text(predicted_sequence.cpu().numpy())\n",
    "\n",
    "    return predicted_text"
   ],
   "id": "27d094057c0bb755",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# main()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to train the speech-to-text model.\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    device = torch.device('cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')  # For Apple Silicon Macs\n",
    "    else:\n",
    "        print(\"No GPU available, using CPU.\")\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Paths - adjust these to your Common Voice dataset location\n",
    "    # data_dir = os.getenv(\"COVOST2_DATASET\")  # Change this path\n",
    "    # train_metadata = f\"{data_dir}/train.tsv\"\n",
    "    # val_metadata = f\"{data_dir}/dev.tsv\"\n",
    "\n",
    "    # Load dataset using Hugging Face\n",
    "    data_dir = os.getenv(\"COVOST2_DATASET\")\n",
    "    cv_4_0 = load_dataset(\n",
    "        \"facebook/covost2\",\n",
    "        \"en_de\",\n",
    "        data_dir=data_dir,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Initialize processors\n",
    "    text_processor = TextProcessor(vocab_size_limit=100)\n",
    "    audio_processor = AudioProcessor()\n",
    "\n",
    "    # Build vocabulary from training data\n",
    "    print(\"Building vocabulary...\")\n",
    "    train_texts = cv_4_0['train']['sentence']\n",
    "    text_processor.build_vocabulary(train_texts)\n",
    "\n",
    "    # Create datasets using the new HuggingFaceDataset class\n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = HuggingFaceDataset(cv_4_0['train'], text_processor, audio_processor)\n",
    "    val_dataset = HuggingFaceDataset(cv_4_0['validation'], text_processor, audio_processor)\n",
    "\n",
    "    \"\"\"# Initialize processors\n",
    "    text_processor = TextProcessor(vocab_size_limit=100)  # Limit vocabulary for demo\n",
    "    audio_processor = AudioProcessor()\n",
    "\n",
    "    # Build vocabulary from training data\n",
    "    print(\"Building vocabulary...\")\n",
    "    train_df = pd.read_csv(train_metadata, sep='\\t')\n",
    "    texts = train_df['sentence'].dropna().tolist()\n",
    "    text_processor.build_vocabulary(texts)\n",
    "\n",
    "    # Create datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = CommonVoiceDataset(data_dir, train_metadata, text_processor, audio_processor)\n",
    "    val_dataset = CommonVoiceDataset(data_dir, val_metadata, text_processor, audio_processor)\"\"\"\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=8,  # Small batch size for demo\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    # Create model\n",
    "    print(\"Creating model...\")\n",
    "    model = Speech2TextModel(\n",
    "        vocab_size=text_processor.vocab_size,\n",
    "        input_size=audio_processor.n_mels,\n",
    "        encoder_hidden=128,  # Smaller for demo\n",
    "        decoder_hidden=128,\n",
    "        num_layers=1\n",
    "    )\n",
    "\n",
    "    print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, train_loader, val_loader, text_processor, num_epochs=1, device=device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "9788d46179ac1e4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')  # For Apple Silicon Macs\n",
    "else:\n",
    "    print(\"No GPU available, using CPU.\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "text_processor = TextProcessor(vocab_size_limit=100)\n",
    "audio_processor = AudioProcessor()\n",
    "\n",
    "# Example prediction\n",
    "print(\"\\nTesting prediction...\")\n",
    "# Use a file from your validation set\n",
    "data_dir = os.getenv(\"COVOST2_DATASET\")\n",
    "sample_audio = f\"{data_dir}/harvard.wav\"  # Change this\n",
    "model = \"./best_model.pth\"  # Load your trained model here\n",
    "if os.path.exists(model):\n",
    "    model = Speech2TextModel(\n",
    "        vocab_size=text_processor.vocab_size,\n",
    "        input_size=audio_processor.n_mels,\n",
    "        encoder_hidden=128,  # Match your training config\n",
    "        decoder_hidden=128,\n",
    "        num_layers=1\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model, map_location=device))\n",
    "else:\n",
    "    print(\"Model file not found. Please train the model first.\")\n",
    "if os.path.exists(sample_audio):\n",
    "    prediction = predict_text(model, sample_audio, text_processor, audio_processor, device)\n",
    "    print(f\"Predicted text: {prediction}\")"
   ],
   "id": "578c92e084365b36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T11:20:51.828899Z",
     "start_time": "2025-08-22T11:20:47.797364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_test_model():\n",
    "    \"\"\"\n",
    "    Load trained model and test on audio file.\n",
    "    \"\"\"\n",
    "    device = torch.device('cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        print(\"No GPU available, using CPU.\")\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load dataset to rebuild vocabulary (needed for text processing)\n",
    "    data_dir = os.getenv(\"COVOST2_DATASET\")\n",
    "    cv_4_0 = load_dataset(\n",
    "        \"facebook/covost2\",\n",
    "        \"en_de\",\n",
    "        data_dir=data_dir,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \"\"\"data_cv4 = next(iter(cv_4_0['test']))\n",
    "\n",
    "    sample_audio = data_cv4['audio']['path'] # f\"{data_dir}/harvard.wav\"  # Replace with actual audio file\n",
    "    print(f\"sample_audio: {sample_audio}\")\"\"\"\n",
    "\n",
    "    # Initialize processors with same settings as training\n",
    "    text_processor = TextProcessor(vocab_size_limit=100)\n",
    "    audio_processor = AudioProcessor()\n",
    "\n",
    "    # Rebuild vocabulary from training data (must match training exactly)\n",
    "    print(\"Building vocabulary...\")\n",
    "    train_texts = cv_4_0['train']['sentence']\n",
    "    text_processor.build_vocabulary(train_texts)\n",
    "\n",
    "    # Create model with same architecture as training\n",
    "    model = Speech2TextModel(\n",
    "        vocab_size=text_processor.vocab_size,\n",
    "        input_size=audio_processor.n_mels,\n",
    "        encoder_hidden=128,  # Must match training config\n",
    "        decoder_hidden=128,\n",
    "        num_layers=1\n",
    "    )\n",
    "\n",
    "    # Load trained weights\n",
    "    model_path = \"./best_model.pth\"\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.to(device)\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "        # Test prediction\n",
    "        sample_audio = f\"{data_dir}/clips/common_voice_en_699711.mp3\"  # Replace with actual audio file\n",
    "        # print(f\"sample_audio: {sample_audio}\")\n",
    "        if os.path.exists(sample_audio):\n",
    "            prediction = predict_text(model, sample_audio, text_processor, audio_processor, device)\n",
    "            print(f\"Predicted text: {prediction}\")\n",
    "        else:\n",
    "            print(f\"Audio file not found: {sample_audio}\")\n",
    "            print(\"Please provide a valid audio file path.\")\n",
    "    else:\n",
    "        print(\"Model file not found. Please train the model first.\")\n",
    "\n",
    "# Call the function\n",
    "load_and_test_model()"
   ],
   "id": "906fb2e9ec5c8cb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Building vocabulary...\n",
      "Vocabulary size: 100\n",
      "Sample characters: ['<PAD>', '<UNK>', '<SOS>', '<EOS>', ' ', 'e', 'a', 't', 'i', 'o', 's', 'n', 'r', 'h', 'l', 'd', 'c', 'u', 'm', 'p']\n",
      "Model loaded successfully!\n",
      "Predicted text: the man in the man in the man in the man in the market and the community.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wg/rl6930hx3n19hn8c6bnp6nmw0000gn/T/ipykernel_54422/2591337291.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ad2afab3feef9cb9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
